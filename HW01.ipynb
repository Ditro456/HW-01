{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ab091f-76fb-461f-9825-1f2145e5b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "np.random.choice(60000, 10)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965cb3ec-d0b1-4639-ae24-53f5c2f6cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from common.trainer import Trainer\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ThreeLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, weight_init_std=0.01):\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)\n",
    "        self.params['b1'] = np.zeros(hidden_size1)\n",
    "        self.params['gamma1'] = np.ones(hidden_size1)\n",
    "        self.params['beta1'] = np.zeros(hidden_size1)\n",
    "        \n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params['b2'] = np.zeros(hidden_size2)\n",
    "        self.params['gamma2'] = np.ones(hidden_size2)\n",
    "        self.params['beta2'] = np.zeros(hidden_size2)\n",
    "        \n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        self.params['gamma3'] = np.ones(output_size)\n",
    "        self.params['beta3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['BatchNorm1'] = BatchNormalization(self.params['gamma1'], self.params['beta1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        \n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['BatchNorm2'] = BatchNormalization(self.params['gamma2'], self.params['beta2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        \n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['BatchNorm3'] = BatchNormalization(self.params['gamma3'], self.params['beta3'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers.values():\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, train_flg=False):\n",
    "        y = self.predict(x, train_flg)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t, train_flg=True)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
    "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
    "        grads['gamma1'] = numerical_gradient(loss_W, self.params['gamma1'])\n",
    "        grads['beta1'] = numerical_gradient(loss_W, self.params['beta1'])\n",
    "        grads['gamma2'] = numerical_gradient(loss_W, self.params['gamma2'])\n",
    "        grads['beta2'] = numerical_gradient(loss_W, self.params['beta2'])\n",
    "        grads['gamma3'] = numerical_gradient(loss_W, self.params['gamma3'])\n",
    "        grads['beta3'] = numerical_gradient(loss_W, self.params['beta3'])\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, train_flg=True)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.layers['Affine1'].dW, 'b1': self.layers['Affine1'].db,\n",
    "            'W2': self.layers['Affine2'].dW, 'b2': self.layers['Affine2'].db,\n",
    "            'W3': self.layers['Affine3'].dW, 'b3': self.layers['Affine3'].db,\n",
    "            'gamma1': self.layers['BatchNorm1'].dgamma, 'beta1': self.layers['BatchNorm1'].dbeta,\n",
    "            'gamma2': self.layers['BatchNorm2'].dgamma, 'beta2': self.layers['BatchNorm2'].dbeta,\n",
    "            'gamma3': self.layers['BatchNorm3'].dgamma, 'beta3': self.layers['BatchNorm3'].dbeta\n",
    "        }\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f10e92-5880-4fef-960b-a50dff9bda55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate 0.001 and hidden size 5...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m network \u001b[38;5;241m=\u001b[39m ThreeLayerNet(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m784\u001b[39m, hidden_size1\u001b[38;5;241m=\u001b[39mhidden_size, hidden_size2\u001b[38;5;241m=\u001b[39mhidden_size, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test, \n\u001b[0;32m     18\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, mini_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     19\u001b[0m                   optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer_param\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr},\n\u001b[0;32m     20\u001b[0m                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     24\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39maccuracy(x_test, t_test)\n\u001b[0;32m     25\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39maccuracy(x_train, t_train)\n",
      "File \u001b[1;32m~\\AIstart\\common\\trainer.py:70\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[1;32m---> 70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     72\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39maccuracy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_test)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[1;32m~\\AIstart\\common\\trainer.py:43\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train[batch_mask]\n\u001b[0;32m     41\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[batch_mask]\n\u001b[1;32m---> 43\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mparams, grads)\n\u001b[0;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n",
      "Cell \u001b[1;32mIn[12], line 82\u001b[0m, in \u001b[0;36mThreeLayerNet.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     dout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[12], line 52\u001b[0m, in \u001b[0;36mThreeLayerNet.loss\u001b[1;34m(self, x, t, train_flg)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 52\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, train_flg)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastLayer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m, in \u001b[0;36mThreeLayerNet.predict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     46\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x, train_flg)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AIstart\\common\\layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.trainer import Trainer\n",
    "\n",
    "learning_rates = np.arange(0.001, 0.02, 0.001)\n",
    "hidden_layer_sizes = np.arange(5, 16)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparameters = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hidden_size in hidden_layer_sizes:\n",
    "        print(f\"Training with learning rate {lr} and hidden size {hidden_size}...\")\n",
    "        \n",
    "        network = ThreeLayerNet(input_size=784, hidden_size1=hidden_size, hidden_size2=hidden_size, output_size=10)\n",
    "\n",
    "        trainer = Trainer(network, x_train, t_train, x_test, t_test, \n",
    "                          epochs=200, mini_batch_size=50,\n",
    "                          optimizer='sgd', optimizer_param={'lr': lr},\n",
    "                          verbose=False)\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        test_accuracy = network.accuracy(x_test, t_test)\n",
    "        train_accuracy = network.accuracy(x_train, t_train)\n",
    "\n",
    "        print(f\"Train accuracy: {train_accuracy}, Test accuracy: {test_accuracy}\")\n",
    "        \n",
    "        if test_accuracy >= 0.95 and train_accuracy >= 0.95:\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                best_hyperparameters = {'learning_rate': lr, 'hidden_size': hidden_size}\n",
    "                best_network = network\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best test accuracy:\", best_accuracy)\n",
    "\n",
    "plt.plot(trainer.train_acc_list, label='Train accuracy')\n",
    "plt.plot(trainer.test_acc_list, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661af963-a150-4a6e-a62a-919ac6f5d7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
